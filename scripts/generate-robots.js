#!/usr/bin/env node

/**
 * Gerador Autom√°tico de robots.txt
 * 
 * Este script gera automaticamente um arquivo robots.txt robusto
 * com configura√ß√µes otimizadas para SEO.
 */

const fs = require('fs');

// Configura√ß√µes
const BASE_URL = 'https://joaoclaudiano.github.io/gittree';
const OUTPUT_FILE = 'robots.txt';

/**
 * Gera o conte√∫do do robots.txt
 */
function generateRobotsTxt() {
    let content = '';
    
    // Cabe√ßalho
    content += '# robots.txt para GitTree\n';
    content += '# Gerado automaticamente\n';
    content += `# ${new Date().toISOString()}\n\n`;
    
    // Configura√ß√£o padr√£o para todos os bots
    content += '# Configura√ß√£o padr√£o\n';
    content += 'User-agent: *\n';
    content += 'Allow: /\n\n';
    
    // Bloquear √°reas administrativas e privadas
    content += '# √Åreas administrativas e privadas\n';
    content += 'Disallow: /admin/\n';
    content += 'Disallow: /private/\n';
    content += 'Disallow: /.git/\n';
    content += 'Disallow: /node_modules/\n';
    content += 'Disallow: /scripts/\n\n';
    
    // Bloquear arquivos t√©cnicos
    content += '# Arquivos t√©cnicos (wildcards)\n';
    content += 'Disallow: *.json\n';
    content += 'Disallow: *.js.map\n';
    content += 'Disallow: *.css.map\n';
    content += 'Disallow: *_config.yml\n\n';
    
    // Bloquear arquivos espec√≠ficos
    content += '# Arquivos espec√≠ficos\n';
    content += 'Disallow: /google*.html\n';
    content += 'Disallow: /404.html\n\n';
    
    // Permitir recursos importantes
    content += '# Permitir recursos importantes\n';
    content += 'Allow: /icons/\n';
    content += 'Allow: /*.css$\n';
    content += 'Allow: /*.js$\n';
    content += 'Allow: /manifest.json\n\n';
    
    // Configura√ß√µes espec√≠ficas por crawler
    content += '# Googlebot\n';
    content += 'User-agent: Googlebot\n';
    content += 'Allow: /\n';
    content += 'Crawl-delay: 0\n\n';
    
    content += '# Bingbot\n';
    content += 'User-agent: Bingbot\n';
    content += 'Allow: /\n';
    content += 'Crawl-delay: 0\n\n';
    
    content += '# DuckDuckBot\n';
    content += 'User-agent: DuckDuckBot\n';
    content += 'Allow: /\n\n';
    
    content += '# Slurp (Yahoo)\n';
    content += 'User-agent: Slurp\n';
    content += 'Allow: /\n\n';
    
    content += '# Baiduspider\n';
    content += 'User-agent: Baiduspider\n';
    content += 'Allow: /\n\n';
    
    content += '# YandexBot\n';
    content += 'User-agent: YandexBot\n';
    content += 'Allow: /\n\n';
    
    // Bloquear bots maliciosos conhecidos
    content += '# Bloquear bots maliciosos\n';
    content += 'User-agent: AhrefsBot\n';
    content += 'Disallow: /\n\n';
    
    content += 'User-agent: SemrushBot\n';
    content += 'Disallow: /\n\n';
    
    content += 'User-agent: MJ12bot\n';
    content += 'Disallow: /\n\n';
    
    content += 'User-agent: DotBot\n';
    content += 'Disallow: /\n\n';
    
    // Sitemap
    content += '# Sitemap\n';
    content += `Sitemap: ${BASE_URL}/sitemap.xml\n`;
    
    return content;
}

/**
 * Fun√ß√£o principal
 */
function main() {
    try {
        console.log('üìù Gerando robots.txt...');
        
        const robotsContent = generateRobotsTxt();
        
        fs.writeFileSync(OUTPUT_FILE, robotsContent, 'utf8');
        
        console.log(`‚úÖ robots.txt gerado com sucesso: ${OUTPUT_FILE}`);
        console.log(`üåê Sitemap URL: ${BASE_URL}/sitemap.xml`);
        console.log('üìã Configura√ß√µes inclu√≠das:');
        console.log('   - Bloqueio de √°reas administrativas');
        console.log('   - Bloqueio de arquivos t√©cnicos');
        console.log('   - Configura√ß√µes espec√≠ficas por crawler');
        console.log('   - Bloqueio de bots maliciosos');
        
    } catch (error) {
        console.error('‚ùå Erro ao gerar robots.txt:', error.message);
        process.exit(1);
    }
}

// Executa o script
if (require.main === module) {
    main();
}

module.exports = { generateRobotsTxt };
